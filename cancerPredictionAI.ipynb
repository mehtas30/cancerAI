{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehtas30/cancerAI/blob/main/cancerPredictionAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPMYhK4MC4VC"
      },
      "source": [
        "#30 attribute with radius, texture,perimenter,area,smoothness etc too see if cancer is benign or malignant\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhnliy77EW_t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "acb9376b-87a6-4ab4-d759-119448a75b8f"
      },
      "source": [
        "df=pd.read_csv (\"cancer_classification.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>radius error</th>\n",
              "      <th>texture error</th>\n",
              "      <th>perimeter error</th>\n",
              "      <th>area error</th>\n",
              "      <th>smoothness error</th>\n",
              "      <th>compactness error</th>\n",
              "      <th>concavity error</th>\n",
              "      <th>concave points error</th>\n",
              "      <th>symmetry error</th>\n",
              "      <th>fractal dimension error</th>\n",
              "      <th>worst radius</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "      <th>benign_0__mal_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   mean radius  mean texture  ...  worst fractal dimension  benign_0__mal_1\n",
              "0        17.99         10.38  ...                  0.11890                0\n",
              "1        20.57         17.77  ...                  0.08902                0\n",
              "2        19.69         21.25  ...                  0.08758                0\n",
              "3        11.42         20.38  ...                  0.17300                0\n",
              "4        20.29         14.34  ...                  0.07678                0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1-hgvBqH37U"
      },
      "source": [
        "#visual graphs\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "N4CNvqZHIBwV",
        "outputId": "50f1ce79-4091-4e34-ea91-fef9fa5de31e"
      },
      "source": [
        "sns.countplot(x=\"benign_0__mal_1\", data=df) #0 benign, 1 malignant ideally want data to be even so AI can learn from both"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7eff52c32a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "Xq3QZb9dJLAB",
        "outputId": "143f4ff4-3d33-416b-e2a0-5c8c2967fc1d"
      },
      "source": [
        "sns.boxplot(x=\"benign_0__mal_1\",y=\"worst radius\", data=df) #not discrete data (using box plot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7eff52b7f438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aIshsaMKSP5",
        "outputId": "1378c5c2-4ccb-401b-b129-49a7078b2f3d"
      },
      "source": [
        "#data split\n",
        "X= df.drop(\"benign_0__mal_1\", axis=1).values #pandas to drop beningn0mal1 colunm (remove it) and assinging it to X\n",
        "y=df[\"benign_0__mal_1\"].values #stores actual labels\n",
        "y.shape #array shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2YSwkhaVyFe",
        "outputId": "0f7e2bfa-6527-4707-fbe1-687b02996e8f"
      },
      "source": [
        "\n",
        "X_train.shape\n",
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(426, 30)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COx_fsKcM0_C"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3URzN5TM9QE"
      },
      "source": [
        "X_train, X_test, y_train, y_test= train_test_split(X,y,test_size=0.25, random_state=101)#25% as \"exam\", random state means same test for every computer it runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHQYNXh1N3M6",
        "outputId": "cf72a98e-fd05-40b9-b75f-418e6ff6aa37"
      },
      "source": [
        "#Data normalization : takes range of values and squishes them into 0-1 for faster processing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler= MinMaxScaler()\n",
        "scaler.fit(X_train) #stores ranges for all colunms"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MinMaxScaler(copy=True, feature_range=(0, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZOvK0OrPWBd"
      },
      "source": [
        "X_train=scaler.transform(X_train)\n",
        "X_test=scaler.transform(X_test)#actually normalizing them (divides by the range found)\n",
        "#y dataset is already 0,1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcDuDs_lQn2i"
      },
      "source": [
        "#architecture\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNx_J0wAQ61M"
      },
      "source": [
        "model= Sequential()\n",
        "model.add(Dense(units=30,activation='relu')) #make layers match number of colunms in x train: input layer\n",
        "model.add(Dense(units=15,activation='relu')) #units can be chosen depending on how much time you want to spend: hidden layer\n",
        "model.add(Dense(units=1,activation='sigmoid')) #output layer \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DO6YDHCoS6aV"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam') #loss function for binary (malignent or benign), adam is for modifing weights of each connection for accuracy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aj6VviQATlgR",
        "outputId": "1d22f487-5498-460c-d577-2a1651661c7d"
      },
      "source": [
        "#model training\n",
        "\n",
        "model.fit(x=X_train, \n",
        "          y=y_train, \n",
        "          epochs=600,\n",
        "          validation_data=(X_test, y_test), verbose=1\n",
        "          )#x question, y answer, epoch=600times, test question/asnwers, shows output\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/600\n",
            "14/14 [==============================] - 2s 14ms/step - loss: 0.7086 - val_loss: 0.6749\n",
            "Epoch 2/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.6680 - val_loss: 0.6421\n",
            "Epoch 3/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.6323 - val_loss: 0.6075\n",
            "Epoch 4/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.5990 - val_loss: 0.5694\n",
            "Epoch 5/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.5557 - val_loss: 0.5266\n",
            "Epoch 6/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.4993 - val_loss: 0.4766\n",
            "Epoch 7/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.4631 - val_loss: 0.4190\n",
            "Epoch 8/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.4139 - val_loss: 0.3617\n",
            "Epoch 9/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.3668 - val_loss: 0.3113\n",
            "Epoch 10/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.3144 - val_loss: 0.2727\n",
            "Epoch 11/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2665 - val_loss: 0.2426\n",
            "Epoch 12/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2477 - val_loss: 0.2195\n",
            "Epoch 13/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2159 - val_loss: 0.2026\n",
            "Epoch 14/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2058 - val_loss: 0.1874\n",
            "Epoch 15/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.2166 - val_loss: 0.1766\n",
            "Epoch 16/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1764 - val_loss: 0.1729\n",
            "Epoch 17/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1820 - val_loss: 0.1601\n",
            "Epoch 18/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1584 - val_loss: 0.1621\n",
            "Epoch 19/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1288 - val_loss: 0.1499\n",
            "Epoch 20/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1438 - val_loss: 0.1454\n",
            "Epoch 21/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1296 - val_loss: 0.1446\n",
            "Epoch 22/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1092 - val_loss: 0.1381\n",
            "Epoch 23/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1297 - val_loss: 0.1358\n",
            "Epoch 24/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1094 - val_loss: 0.1301\n",
            "Epoch 25/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1213 - val_loss: 0.1366\n",
            "Epoch 26/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1202 - val_loss: 0.1260\n",
            "Epoch 27/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1025 - val_loss: 0.1366\n",
            "Epoch 28/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1038 - val_loss: 0.1238\n",
            "Epoch 29/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.1066 - val_loss: 0.1233\n",
            "Epoch 30/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0966 - val_loss: 0.1254\n",
            "Epoch 31/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0897 - val_loss: 0.1204\n",
            "Epoch 32/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0832 - val_loss: 0.1216\n",
            "Epoch 33/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0974 - val_loss: 0.1201\n",
            "Epoch 34/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0770 - val_loss: 0.1259\n",
            "Epoch 35/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0966 - val_loss: 0.1174\n",
            "Epoch 36/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0821 - val_loss: 0.1213\n",
            "Epoch 37/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0893 - val_loss: 0.1180\n",
            "Epoch 38/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0760 - val_loss: 0.1180\n",
            "Epoch 39/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0674 - val_loss: 0.1177\n",
            "Epoch 40/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0738 - val_loss: 0.1175\n",
            "Epoch 41/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0953 - val_loss: 0.1159\n",
            "Epoch 42/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0655 - val_loss: 0.1208\n",
            "Epoch 43/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0923 - val_loss: 0.1147\n",
            "Epoch 44/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0725 - val_loss: 0.1158\n",
            "Epoch 45/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0715 - val_loss: 0.1176\n",
            "Epoch 46/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0750 - val_loss: 0.1165\n",
            "Epoch 47/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0762 - val_loss: 0.1154\n",
            "Epoch 48/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0677 - val_loss: 0.1154\n",
            "Epoch 49/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0688 - val_loss: 0.1177\n",
            "Epoch 50/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0749 - val_loss: 0.1124\n",
            "Epoch 51/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0661 - val_loss: 0.1173\n",
            "Epoch 52/600\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.0680 - val_loss: 0.1151\n",
            "Epoch 53/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0694 - val_loss: 0.1148\n",
            "Epoch 54/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0680 - val_loss: 0.1155\n",
            "Epoch 55/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0563 - val_loss: 0.1163\n",
            "Epoch 56/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0651 - val_loss: 0.1164\n",
            "Epoch 57/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0439 - val_loss: 0.1178\n",
            "Epoch 58/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0554 - val_loss: 0.1181\n",
            "Epoch 59/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0576 - val_loss: 0.1109\n",
            "Epoch 60/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0540 - val_loss: 0.1211\n",
            "Epoch 61/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0502 - val_loss: 0.1121\n",
            "Epoch 62/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0606 - val_loss: 0.1145\n",
            "Epoch 63/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0576 - val_loss: 0.1195\n",
            "Epoch 64/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0484 - val_loss: 0.1177\n",
            "Epoch 65/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0561 - val_loss: 0.1230\n",
            "Epoch 66/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0580 - val_loss: 0.1133\n",
            "Epoch 67/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0521 - val_loss: 0.1198\n",
            "Epoch 68/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0498 - val_loss: 0.1201\n",
            "Epoch 69/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0561 - val_loss: 0.1134\n",
            "Epoch 70/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0613 - val_loss: 0.1355\n",
            "Epoch 71/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0546 - val_loss: 0.1127\n",
            "Epoch 72/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0706 - val_loss: 0.1157\n",
            "Epoch 73/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0700 - val_loss: 0.1156\n",
            "Epoch 74/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0445 - val_loss: 0.1204\n",
            "Epoch 75/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0439 - val_loss: 0.1159\n",
            "Epoch 76/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0548 - val_loss: 0.1169\n",
            "Epoch 77/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0431 - val_loss: 0.1303\n",
            "Epoch 78/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0666 - val_loss: 0.1214\n",
            "Epoch 79/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0495 - val_loss: 0.1192\n",
            "Epoch 80/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0535 - val_loss: 0.1213\n",
            "Epoch 81/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0532 - val_loss: 0.1239\n",
            "Epoch 82/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0565 - val_loss: 0.1211\n",
            "Epoch 83/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0523 - val_loss: 0.1209\n",
            "Epoch 84/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0418 - val_loss: 0.1196\n",
            "Epoch 85/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0632 - val_loss: 0.1223\n",
            "Epoch 86/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0408 - val_loss: 0.1229\n",
            "Epoch 87/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0527 - val_loss: 0.1202\n",
            "Epoch 88/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0546 - val_loss: 0.1204\n",
            "Epoch 89/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0535 - val_loss: 0.1238\n",
            "Epoch 90/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0539 - val_loss: 0.1208\n",
            "Epoch 91/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0471 - val_loss: 0.1254\n",
            "Epoch 92/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0531 - val_loss: 0.1224\n",
            "Epoch 93/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0546 - val_loss: 0.1209\n",
            "Epoch 94/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0651 - val_loss: 0.1291\n",
            "Epoch 95/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0465 - val_loss: 0.1277\n",
            "Epoch 96/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0480 - val_loss: 0.1255\n",
            "Epoch 97/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0499 - val_loss: 0.1302\n",
            "Epoch 98/600\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.0414 - val_loss: 0.1256\n",
            "Epoch 99/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0417 - val_loss: 0.1192\n",
            "Epoch 100/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0669 - val_loss: 0.1197\n",
            "Epoch 101/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0634 - val_loss: 0.1257\n",
            "Epoch 102/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0423 - val_loss: 0.1350\n",
            "Epoch 103/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0611 - val_loss: 0.1215\n",
            "Epoch 104/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0378 - val_loss: 0.1324\n",
            "Epoch 105/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0543 - val_loss: 0.1253\n",
            "Epoch 106/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0625 - val_loss: 0.1206\n",
            "Epoch 107/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0410 - val_loss: 0.1292\n",
            "Epoch 108/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0363 - val_loss: 0.1302\n",
            "Epoch 109/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0497 - val_loss: 0.1262\n",
            "Epoch 110/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0484 - val_loss: 0.1324\n",
            "Epoch 111/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0462 - val_loss: 0.1244\n",
            "Epoch 112/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0685 - val_loss: 0.1270\n",
            "Epoch 113/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0574 - val_loss: 0.1339\n",
            "Epoch 114/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0339 - val_loss: 0.1256\n",
            "Epoch 115/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0596 - val_loss: 0.1273\n",
            "Epoch 116/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0575 - val_loss: 0.1311\n",
            "Epoch 117/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0388 - val_loss: 0.1311\n",
            "Epoch 118/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0532 - val_loss: 0.1290\n",
            "Epoch 119/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0381 - val_loss: 0.1472\n",
            "Epoch 120/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0439 - val_loss: 0.1250\n",
            "Epoch 121/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0566 - val_loss: 0.1274\n",
            "Epoch 122/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0376 - val_loss: 0.1322\n",
            "Epoch 123/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.1316\n",
            "Epoch 124/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0468 - val_loss: 0.1308\n",
            "Epoch 125/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0407 - val_loss: 0.1328\n",
            "Epoch 126/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0522 - val_loss: 0.1227\n",
            "Epoch 127/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0747 - val_loss: 0.1342\n",
            "Epoch 128/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0493 - val_loss: 0.1242\n",
            "Epoch 129/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0419 - val_loss: 0.1387\n",
            "Epoch 130/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0493 - val_loss: 0.1435\n",
            "Epoch 131/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0446 - val_loss: 0.1273\n",
            "Epoch 132/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0508 - val_loss: 0.1261\n",
            "Epoch 133/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0639 - val_loss: 0.1342\n",
            "Epoch 134/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0477 - val_loss: 0.1289\n",
            "Epoch 135/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0406 - val_loss: 0.1365\n",
            "Epoch 136/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0595 - val_loss: 0.1316\n",
            "Epoch 137/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0310 - val_loss: 0.1314\n",
            "Epoch 138/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0430 - val_loss: 0.1381\n",
            "Epoch 139/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0452 - val_loss: 0.1339\n",
            "Epoch 140/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0494 - val_loss: 0.1329\n",
            "Epoch 141/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0365 - val_loss: 0.1332\n",
            "Epoch 142/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0552 - val_loss: 0.1353\n",
            "Epoch 143/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0421 - val_loss: 0.1292\n",
            "Epoch 144/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0673 - val_loss: 0.1300\n",
            "Epoch 145/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0433 - val_loss: 0.1414\n",
            "Epoch 146/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0383 - val_loss: 0.1434\n",
            "Epoch 147/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0415 - val_loss: 0.1349\n",
            "Epoch 148/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0383 - val_loss: 0.1493\n",
            "Epoch 149/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0475 - val_loss: 0.1344\n",
            "Epoch 150/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0533 - val_loss: 0.1330\n",
            "Epoch 151/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0372 - val_loss: 0.1463\n",
            "Epoch 152/600\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.0430 - val_loss: 0.1396\n",
            "Epoch 153/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0435 - val_loss: 0.1354\n",
            "Epoch 154/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0331 - val_loss: 0.1509\n",
            "Epoch 155/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0487 - val_loss: 0.1386\n",
            "Epoch 156/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0348 - val_loss: 0.1358\n",
            "Epoch 157/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0435 - val_loss: 0.1382\n",
            "Epoch 158/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0382 - val_loss: 0.1405\n",
            "Epoch 159/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0525 - val_loss: 0.1364\n",
            "Epoch 160/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0424 - val_loss: 0.1447\n",
            "Epoch 161/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0398 - val_loss: 0.1366\n",
            "Epoch 162/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0325 - val_loss: 0.1531\n",
            "Epoch 163/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0326 - val_loss: 0.1420\n",
            "Epoch 164/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0397 - val_loss: 0.1416\n",
            "Epoch 165/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0395 - val_loss: 0.1351\n",
            "Epoch 166/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0459 - val_loss: 0.1493\n",
            "Epoch 167/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0486 - val_loss: 0.1347\n",
            "Epoch 168/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0400 - val_loss: 0.1435\n",
            "Epoch 169/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0662 - val_loss: 0.1364\n",
            "Epoch 170/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0423 - val_loss: 0.1432\n",
            "Epoch 171/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0621 - val_loss: 0.1306\n",
            "Epoch 172/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0525 - val_loss: 0.1548\n",
            "Epoch 173/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0406 - val_loss: 0.1486\n",
            "Epoch 174/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0519 - val_loss: 0.1307\n",
            "Epoch 175/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0573 - val_loss: 0.1494\n",
            "Epoch 176/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0493 - val_loss: 0.1425\n",
            "Epoch 177/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0458 - val_loss: 0.1438\n",
            "Epoch 178/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0468 - val_loss: 0.1447\n",
            "Epoch 179/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0448 - val_loss: 0.1521\n",
            "Epoch 180/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0415 - val_loss: 0.1363\n",
            "Epoch 181/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0456 - val_loss: 0.1584\n",
            "Epoch 182/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0562 - val_loss: 0.1369\n",
            "Epoch 183/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0333 - val_loss: 0.1517\n",
            "Epoch 184/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0391 - val_loss: 0.1494\n",
            "Epoch 185/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0396 - val_loss: 0.1356\n",
            "Epoch 186/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0365 - val_loss: 0.1656\n",
            "Epoch 187/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0409 - val_loss: 0.1372\n",
            "Epoch 188/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0424 - val_loss: 0.1632\n",
            "Epoch 189/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0592 - val_loss: 0.1361\n",
            "Epoch 190/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0419 - val_loss: 0.1594\n",
            "Epoch 191/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0528 - val_loss: 0.1358\n",
            "Epoch 192/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0571 - val_loss: 0.1377\n",
            "Epoch 193/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0388 - val_loss: 0.1621\n",
            "Epoch 194/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0391 - val_loss: 0.1419\n",
            "Epoch 195/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0368 - val_loss: 0.1524\n",
            "Epoch 196/600\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.0371 - val_loss: 0.1587\n",
            "Epoch 197/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0460 - val_loss: 0.1569\n",
            "Epoch 198/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0336 - val_loss: 0.1531\n",
            "Epoch 199/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0358 - val_loss: 0.1535\n",
            "Epoch 200/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0385 - val_loss: 0.1461\n",
            "Epoch 201/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0403 - val_loss: 0.1537\n",
            "Epoch 202/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0416 - val_loss: 0.1471\n",
            "Epoch 203/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0468 - val_loss: 0.1462\n",
            "Epoch 204/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0311 - val_loss: 0.1684\n",
            "Epoch 205/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0369 - val_loss: 0.1521\n",
            "Epoch 206/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0341 - val_loss: 0.1529\n",
            "Epoch 207/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0335 - val_loss: 0.1528\n",
            "Epoch 208/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0623 - val_loss: 0.1443\n",
            "Epoch 209/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0277 - val_loss: 0.1582\n",
            "Epoch 210/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0360 - val_loss: 0.1546\n",
            "Epoch 211/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0418 - val_loss: 0.1518\n",
            "Epoch 212/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0385 - val_loss: 0.1476\n",
            "Epoch 213/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0380 - val_loss: 0.1635\n",
            "Epoch 214/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0313 - val_loss: 0.1464\n",
            "Epoch 215/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.1547\n",
            "Epoch 216/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0380 - val_loss: 0.1692\n",
            "Epoch 217/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0415 - val_loss: 0.1630\n",
            "Epoch 218/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0588 - val_loss: 0.1469\n",
            "Epoch 219/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0463 - val_loss: 0.1510\n",
            "Epoch 220/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0306 - val_loss: 0.1662\n",
            "Epoch 221/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0376 - val_loss: 0.1592\n",
            "Epoch 222/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0368 - val_loss: 0.1615\n",
            "Epoch 223/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0455 - val_loss: 0.1606\n",
            "Epoch 224/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0375 - val_loss: 0.1485\n",
            "Epoch 225/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0367 - val_loss: 0.1828\n",
            "Epoch 226/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0554 - val_loss: 0.1465\n",
            "Epoch 227/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0456 - val_loss: 0.1662\n",
            "Epoch 228/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0413 - val_loss: 0.1536\n",
            "Epoch 229/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0242 - val_loss: 0.1512\n",
            "Epoch 230/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0716 - val_loss: 0.1522\n",
            "Epoch 231/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0563 - val_loss: 0.1620\n",
            "Epoch 232/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0360 - val_loss: 0.1680\n",
            "Epoch 233/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0307 - val_loss: 0.1547\n",
            "Epoch 234/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0327 - val_loss: 0.1617\n",
            "Epoch 235/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0471 - val_loss: 0.1433\n",
            "Epoch 236/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0464 - val_loss: 0.1633\n",
            "Epoch 237/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0322 - val_loss: 0.1495\n",
            "Epoch 238/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0405 - val_loss: 0.1620\n",
            "Epoch 239/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0532 - val_loss: 0.1462\n",
            "Epoch 240/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0323 - val_loss: 0.1551\n",
            "Epoch 241/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0467 - val_loss: 0.1514\n",
            "Epoch 242/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0246 - val_loss: 0.1599\n",
            "Epoch 243/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0311 - val_loss: 0.1569\n",
            "Epoch 244/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0397 - val_loss: 0.1620\n",
            "Epoch 245/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0323 - val_loss: 0.1592\n",
            "Epoch 246/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0413 - val_loss: 0.1471\n",
            "Epoch 247/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0303 - val_loss: 0.1662\n",
            "Epoch 248/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0373 - val_loss: 0.1483\n",
            "Epoch 249/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0377 - val_loss: 0.1628\n",
            "Epoch 250/600\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.0316 - val_loss: 0.1532\n",
            "Epoch 251/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0536 - val_loss: 0.1601\n",
            "Epoch 252/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0380 - val_loss: 0.1584\n",
            "Epoch 253/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0465 - val_loss: 0.1615\n",
            "Epoch 254/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0375 - val_loss: 0.1498\n",
            "Epoch 255/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0448 - val_loss: 0.1590\n",
            "Epoch 256/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0410 - val_loss: 0.1561\n",
            "Epoch 257/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0460 - val_loss: 0.1550\n",
            "Epoch 258/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0530 - val_loss: 0.1481\n",
            "Epoch 259/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0441 - val_loss: 0.1678\n",
            "Epoch 260/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0420 - val_loss: 0.1541\n",
            "Epoch 261/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0435 - val_loss: 0.1580\n",
            "Epoch 262/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0312 - val_loss: 0.1584\n",
            "Epoch 263/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0308 - val_loss: 0.1558\n",
            "Epoch 264/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0304 - val_loss: 0.1586\n",
            "Epoch 265/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.1611\n",
            "Epoch 266/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0580 - val_loss: 0.1527\n",
            "Epoch 267/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0334 - val_loss: 0.1738\n",
            "Epoch 268/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0454 - val_loss: 0.1596\n",
            "Epoch 269/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0517 - val_loss: 0.1568\n",
            "Epoch 270/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0382 - val_loss: 0.1639\n",
            "Epoch 271/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0337 - val_loss: 0.1626\n",
            "Epoch 272/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0441 - val_loss: 0.1731\n",
            "Epoch 273/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0415 - val_loss: 0.1612\n",
            "Epoch 274/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0409 - val_loss: 0.1644\n",
            "Epoch 275/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0377 - val_loss: 0.1555\n",
            "Epoch 276/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0380 - val_loss: 0.1706\n",
            "Epoch 277/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0305 - val_loss: 0.1609\n",
            "Epoch 278/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0280 - val_loss: 0.1690\n",
            "Epoch 279/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0339 - val_loss: 0.1640\n",
            "Epoch 280/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0405 - val_loss: 0.1682\n",
            "Epoch 281/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0329 - val_loss: 0.1659\n",
            "Epoch 282/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0409 - val_loss: 0.1444\n",
            "Epoch 283/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0397 - val_loss: 0.2059\n",
            "Epoch 284/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0384 - val_loss: 0.1509\n",
            "Epoch 285/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0339 - val_loss: 0.1829\n",
            "Epoch 286/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0359 - val_loss: 0.1690\n",
            "Epoch 287/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0373 - val_loss: 0.1530\n",
            "Epoch 288/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0493 - val_loss: 0.1628\n",
            "Epoch 289/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0443 - val_loss: 0.1630\n",
            "Epoch 290/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0282 - val_loss: 0.1718\n",
            "Epoch 291/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0483 - val_loss: 0.1636\n",
            "Epoch 292/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0369 - val_loss: 0.1564\n",
            "Epoch 293/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0286 - val_loss: 0.1641\n",
            "Epoch 294/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0321 - val_loss: 0.1650\n",
            "Epoch 295/600\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 0.0389 - val_loss: 0.1751\n",
            "Epoch 296/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0314 - val_loss: 0.1693\n",
            "Epoch 297/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0376 - val_loss: 0.1648\n",
            "Epoch 298/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0301 - val_loss: 0.1693\n",
            "Epoch 299/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0453 - val_loss: 0.1606\n",
            "Epoch 300/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0408 - val_loss: 0.1683\n",
            "Epoch 301/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0415 - val_loss: 0.1657\n",
            "Epoch 302/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0317 - val_loss: 0.2003\n",
            "Epoch 303/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0402 - val_loss: 0.1685\n",
            "Epoch 304/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0301 - val_loss: 0.1734\n",
            "Epoch 305/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0346 - val_loss: 0.1651\n",
            "Epoch 306/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0302 - val_loss: 0.1678\n",
            "Epoch 307/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.1724\n",
            "Epoch 308/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0320 - val_loss: 0.1856\n",
            "Epoch 309/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0412 - val_loss: 0.1565\n",
            "Epoch 310/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0245 - val_loss: 0.1779\n",
            "Epoch 311/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0388 - val_loss: 0.1632\n",
            "Epoch 312/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0430 - val_loss: 0.1710\n",
            "Epoch 313/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0311 - val_loss: 0.1687\n",
            "Epoch 314/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0283 - val_loss: 0.1802\n",
            "Epoch 315/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0388 - val_loss: 0.1655\n",
            "Epoch 316/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0369 - val_loss: 0.1848\n",
            "Epoch 317/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0304 - val_loss: 0.1606\n",
            "Epoch 318/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0456 - val_loss: 0.1686\n",
            "Epoch 319/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0357 - val_loss: 0.1604\n",
            "Epoch 320/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0297 - val_loss: 0.1775\n",
            "Epoch 321/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0303 - val_loss: 0.1761\n",
            "Epoch 322/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0381 - val_loss: 0.1739\n",
            "Epoch 323/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0367 - val_loss: 0.1612\n",
            "Epoch 324/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0303 - val_loss: 0.1938\n",
            "Epoch 325/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0320 - val_loss: 0.1727\n",
            "Epoch 326/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0347 - val_loss: 0.1722\n",
            "Epoch 327/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0353 - val_loss: 0.1848\n",
            "Epoch 328/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.1632\n",
            "Epoch 329/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0308 - val_loss: 0.1789\n",
            "Epoch 330/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0308 - val_loss: 0.1751\n",
            "Epoch 331/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0259 - val_loss: 0.1690\n",
            "Epoch 332/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0334 - val_loss: 0.1830\n",
            "Epoch 333/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0306 - val_loss: 0.1723\n",
            "Epoch 334/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0422 - val_loss: 0.1686\n",
            "Epoch 335/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0227 - val_loss: 0.1837\n",
            "Epoch 336/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0245 - val_loss: 0.1728\n",
            "Epoch 337/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0410 - val_loss: 0.1800\n",
            "Epoch 338/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0355 - val_loss: 0.1594\n",
            "Epoch 339/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.2001\n",
            "Epoch 340/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0340 - val_loss: 0.1611\n",
            "Epoch 341/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0483 - val_loss: 0.1632\n",
            "Epoch 342/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0265 - val_loss: 0.1777\n",
            "Epoch 343/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0322 - val_loss: 0.1714\n",
            "Epoch 344/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0394 - val_loss: 0.1893\n",
            "Epoch 345/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0372 - val_loss: 0.1787\n",
            "Epoch 346/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0353 - val_loss: 0.1892\n",
            "Epoch 347/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0249 - val_loss: 0.1716\n",
            "Epoch 348/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0309 - val_loss: 0.1980\n",
            "Epoch 349/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0343 - val_loss: 0.1696\n",
            "Epoch 350/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0390 - val_loss: 0.1793\n",
            "Epoch 351/600\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 0.0370 - val_loss: 0.1641\n",
            "Epoch 352/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0397 - val_loss: 0.1875\n",
            "Epoch 353/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0299 - val_loss: 0.1833\n",
            "Epoch 354/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0393 - val_loss: 0.1678\n",
            "Epoch 355/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0321 - val_loss: 0.2008\n",
            "Epoch 356/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0270 - val_loss: 0.1836\n",
            "Epoch 357/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0265 - val_loss: 0.1808\n",
            "Epoch 358/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.1819\n",
            "Epoch 359/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0348 - val_loss: 0.1741\n",
            "Epoch 360/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0245 - val_loss: 0.1874\n",
            "Epoch 361/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0326 - val_loss: 0.2028\n",
            "Epoch 362/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0319 - val_loss: 0.1926\n",
            "Epoch 363/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0336 - val_loss: 0.1959\n",
            "Epoch 364/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0351 - val_loss: 0.1799\n",
            "Epoch 365/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0341 - val_loss: 0.1729\n",
            "Epoch 366/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.1749\n",
            "Epoch 367/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.1747\n",
            "Epoch 368/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0278 - val_loss: 0.2061\n",
            "Epoch 369/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0337 - val_loss: 0.1754\n",
            "Epoch 370/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0235 - val_loss: 0.2028\n",
            "Epoch 371/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0279 - val_loss: 0.1655\n",
            "Epoch 372/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0295 - val_loss: 0.1933\n",
            "Epoch 373/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0423 - val_loss: 0.1614\n",
            "Epoch 374/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0339 - val_loss: 0.1970\n",
            "Epoch 375/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0325 - val_loss: 0.1660\n",
            "Epoch 376/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0276 - val_loss: 0.2242\n",
            "Epoch 377/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0347 - val_loss: 0.1557\n",
            "Epoch 378/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0322 - val_loss: 0.2317\n",
            "Epoch 379/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0540 - val_loss: 0.1588\n",
            "Epoch 380/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0399 - val_loss: 0.2017\n",
            "Epoch 381/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0263 - val_loss: 0.1780\n",
            "Epoch 382/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0242 - val_loss: 0.1877\n",
            "Epoch 383/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0359 - val_loss: 0.1993\n",
            "Epoch 384/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0336 - val_loss: 0.1765\n",
            "Epoch 385/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0348 - val_loss: 0.2096\n",
            "Epoch 386/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0288 - val_loss: 0.1805\n",
            "Epoch 387/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0325 - val_loss: 0.1954\n",
            "Epoch 388/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0271 - val_loss: 0.1797\n",
            "Epoch 389/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0242 - val_loss: 0.1949\n",
            "Epoch 390/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0310 - val_loss: 0.1755\n",
            "Epoch 391/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0314 - val_loss: 0.2116\n",
            "Epoch 392/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0271 - val_loss: 0.1669\n",
            "Epoch 393/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0238 - val_loss: 0.2068\n",
            "Epoch 394/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0318 - val_loss: 0.1927\n",
            "Epoch 395/600\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 0.0351 - val_loss: 0.1759\n",
            "Epoch 396/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0271 - val_loss: 0.2333\n",
            "Epoch 397/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0373 - val_loss: 0.1600\n",
            "Epoch 398/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0358 - val_loss: 0.2083\n",
            "Epoch 399/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0265 - val_loss: 0.1688\n",
            "Epoch 400/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0386 - val_loss: 0.1891\n",
            "Epoch 401/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0299 - val_loss: 0.1875\n",
            "Epoch 402/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0306 - val_loss: 0.1852\n",
            "Epoch 403/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0237 - val_loss: 0.1900\n",
            "Epoch 404/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0361 - val_loss: 0.1985\n",
            "Epoch 405/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0317 - val_loss: 0.1832\n",
            "Epoch 406/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0211 - val_loss: 0.2132\n",
            "Epoch 407/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.2004\n",
            "Epoch 408/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0338 - val_loss: 0.1986\n",
            "Epoch 409/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0230 - val_loss: 0.1836\n",
            "Epoch 410/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0263 - val_loss: 0.1847\n",
            "Epoch 411/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0333 - val_loss: 0.2030\n",
            "Epoch 412/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0285 - val_loss: 0.1903\n",
            "Epoch 413/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0221 - val_loss: 0.2175\n",
            "Epoch 414/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0372 - val_loss: 0.1871\n",
            "Epoch 415/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0216 - val_loss: 0.1936\n",
            "Epoch 416/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0328 - val_loss: 0.1802\n",
            "Epoch 417/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0294 - val_loss: 0.2358\n",
            "Epoch 418/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0419 - val_loss: 0.1987\n",
            "Epoch 419/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0275 - val_loss: 0.1926\n",
            "Epoch 420/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0318 - val_loss: 0.1874\n",
            "Epoch 421/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0250 - val_loss: 0.1965\n",
            "Epoch 422/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0248 - val_loss: 0.1948\n",
            "Epoch 423/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0271 - val_loss: 0.1991\n",
            "Epoch 424/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0306 - val_loss: 0.1633\n",
            "Epoch 425/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0422 - val_loss: 0.2095\n",
            "Epoch 426/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0224 - val_loss: 0.1780\n",
            "Epoch 427/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.2084\n",
            "Epoch 428/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0339 - val_loss: 0.1980\n",
            "Epoch 429/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0243 - val_loss: 0.1965\n",
            "Epoch 430/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0295 - val_loss: 0.1816\n",
            "Epoch 431/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0242 - val_loss: 0.2130\n",
            "Epoch 432/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0247 - val_loss: 0.1702\n",
            "Epoch 433/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0195 - val_loss: 0.2259\n",
            "Epoch 434/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0309 - val_loss: 0.1796\n",
            "Epoch 435/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0345 - val_loss: 0.2293\n",
            "Epoch 436/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0287 - val_loss: 0.2075\n",
            "Epoch 437/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0213 - val_loss: 0.1882\n",
            "Epoch 438/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0335 - val_loss: 0.2131\n",
            "Epoch 439/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 0.1971\n",
            "Epoch 440/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0323 - val_loss: 0.2008\n",
            "Epoch 441/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0263 - val_loss: 0.2112\n",
            "Epoch 442/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0341 - val_loss: 0.1937\n",
            "Epoch 443/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0293 - val_loss: 0.1979\n",
            "Epoch 444/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0319 - val_loss: 0.1828\n",
            "Epoch 445/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0284 - val_loss: 0.2159\n",
            "Epoch 446/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0235 - val_loss: 0.1989\n",
            "Epoch 447/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0292 - val_loss: 0.2008\n",
            "Epoch 448/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0314 - val_loss: 0.1747\n",
            "Epoch 449/600\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 0.0367 - val_loss: 0.1990\n",
            "Epoch 450/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0314 - val_loss: 0.2421\n",
            "Epoch 451/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0288 - val_loss: 0.1622\n",
            "Epoch 452/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0318 - val_loss: 0.2556\n",
            "Epoch 453/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.1770\n",
            "Epoch 454/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0350 - val_loss: 0.2069\n",
            "Epoch 455/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0273 - val_loss: 0.1723\n",
            "Epoch 456/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0361 - val_loss: 0.2330\n",
            "Epoch 457/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0338 - val_loss: 0.1712\n",
            "Epoch 458/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0313 - val_loss: 0.2536\n",
            "Epoch 459/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0288 - val_loss: 0.1689\n",
            "Epoch 460/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0320 - val_loss: 0.2325\n",
            "Epoch 461/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0241 - val_loss: 0.2035\n",
            "Epoch 462/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0287 - val_loss: 0.1772\n",
            "Epoch 463/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0353 - val_loss: 0.2341\n",
            "Epoch 464/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0239 - val_loss: 0.1772\n",
            "Epoch 465/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0282 - val_loss: 0.2086\n",
            "Epoch 466/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0252 - val_loss: 0.2336\n",
            "Epoch 467/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0247 - val_loss: 0.2006\n",
            "Epoch 468/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0187 - val_loss: 0.2016\n",
            "Epoch 469/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0179 - val_loss: 0.2032\n",
            "Epoch 470/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0213 - val_loss: 0.1871\n",
            "Epoch 471/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0313 - val_loss: 0.2215\n",
            "Epoch 472/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0234 - val_loss: 0.2389\n",
            "Epoch 473/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0209 - val_loss: 0.1845\n",
            "Epoch 474/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0235 - val_loss: 0.2341\n",
            "Epoch 475/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0297 - val_loss: 0.1913\n",
            "Epoch 476/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0262 - val_loss: 0.2271\n",
            "Epoch 477/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0168 - val_loss: 0.1916\n",
            "Epoch 478/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0303 - val_loss: 0.2244\n",
            "Epoch 479/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0212 - val_loss: 0.1984\n",
            "Epoch 480/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0237 - val_loss: 0.2252\n",
            "Epoch 481/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0304 - val_loss: 0.2023\n",
            "Epoch 482/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0190 - val_loss: 0.2059\n",
            "Epoch 483/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0221 - val_loss: 0.2142\n",
            "Epoch 484/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0172 - val_loss: 0.2203\n",
            "Epoch 485/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0285 - val_loss: 0.1881\n",
            "Epoch 486/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0253 - val_loss: 0.2178\n",
            "Epoch 487/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0282 - val_loss: 0.2080\n",
            "Epoch 488/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0211 - val_loss: 0.2102\n",
            "Epoch 489/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0273 - val_loss: 0.2108\n",
            "Epoch 490/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0318 - val_loss: 0.2268\n",
            "Epoch 491/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0249 - val_loss: 0.2180\n",
            "Epoch 492/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0261 - val_loss: 0.2548\n",
            "Epoch 493/600\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 0.0319 - val_loss: 0.1854\n",
            "Epoch 494/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0328 - val_loss: 0.1980\n",
            "Epoch 495/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0241 - val_loss: 0.2208\n",
            "Epoch 496/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0190 - val_loss: 0.1846\n",
            "Epoch 497/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0372 - val_loss: 0.2028\n",
            "Epoch 498/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0177 - val_loss: 0.2044\n",
            "Epoch 499/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0246 - val_loss: 0.2326\n",
            "Epoch 500/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0183 - val_loss: 0.2019\n",
            "Epoch 501/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0222 - val_loss: 0.2262\n",
            "Epoch 502/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0323 - val_loss: 0.2017\n",
            "Epoch 503/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0234 - val_loss: 0.2222\n",
            "Epoch 504/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0207 - val_loss: 0.2132\n",
            "Epoch 505/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0225 - val_loss: 0.2182\n",
            "Epoch 506/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0262 - val_loss: 0.1929\n",
            "Epoch 507/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0350 - val_loss: 0.1898\n",
            "Epoch 508/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0311 - val_loss: 0.2131\n",
            "Epoch 509/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0235 - val_loss: 0.2126\n",
            "Epoch 510/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0182 - val_loss: 0.2378\n",
            "Epoch 511/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0210 - val_loss: 0.1951\n",
            "Epoch 512/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0290 - val_loss: 0.2311\n",
            "Epoch 513/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0203 - val_loss: 0.1983\n",
            "Epoch 514/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0215 - val_loss: 0.2525\n",
            "Epoch 515/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0202 - val_loss: 0.1899\n",
            "Epoch 516/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0320 - val_loss: 0.2156\n",
            "Epoch 517/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0306 - val_loss: 0.2293\n",
            "Epoch 518/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0168 - val_loss: 0.2526\n",
            "Epoch 519/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0266 - val_loss: 0.1854\n",
            "Epoch 520/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0358 - val_loss: 0.2504\n",
            "Epoch 521/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0202 - val_loss: 0.1952\n",
            "Epoch 522/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0288 - val_loss: 0.2443\n",
            "Epoch 523/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0239 - val_loss: 0.1996\n",
            "Epoch 524/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0274 - val_loss: 0.1850\n",
            "Epoch 525/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0255 - val_loss: 0.2562\n",
            "Epoch 526/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0315 - val_loss: 0.2003\n",
            "Epoch 527/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0257 - val_loss: 0.2289\n",
            "Epoch 528/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0222 - val_loss: 0.1954\n",
            "Epoch 529/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0235 - val_loss: 0.2346\n",
            "Epoch 530/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0231 - val_loss: 0.2562\n",
            "Epoch 531/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0245 - val_loss: 0.1940\n",
            "Epoch 532/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0265 - val_loss: 0.2326\n",
            "Epoch 533/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0186 - val_loss: 0.2019\n",
            "Epoch 534/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0175 - val_loss: 0.2407\n",
            "Epoch 535/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0235 - val_loss: 0.2066\n",
            "Epoch 536/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0213 - val_loss: 0.2387\n",
            "Epoch 537/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0184 - val_loss: 0.2004\n",
            "Epoch 538/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0171 - val_loss: 0.2583\n",
            "Epoch 539/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0209 - val_loss: 0.2073\n",
            "Epoch 540/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0356 - val_loss: 0.1956\n",
            "Epoch 541/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0285 - val_loss: 0.2273\n",
            "Epoch 542/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0192 - val_loss: 0.2256\n",
            "Epoch 543/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0187 - val_loss: 0.2331\n",
            "Epoch 544/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0180 - val_loss: 0.2607\n",
            "Epoch 545/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0303 - val_loss: 0.2084\n",
            "Epoch 546/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0310 - val_loss: 0.2400\n",
            "Epoch 547/600\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 0.0209 - val_loss: 0.2306\n",
            "Epoch 548/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0246 - val_loss: 0.2157\n",
            "Epoch 549/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0248 - val_loss: 0.1882\n",
            "Epoch 550/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0354 - val_loss: 0.2319\n",
            "Epoch 551/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0196 - val_loss: 0.2352\n",
            "Epoch 552/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0300 - val_loss: 0.1980\n",
            "Epoch 553/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0300 - val_loss: 0.2436\n",
            "Epoch 554/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0200 - val_loss: 0.2277\n",
            "Epoch 555/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0253 - val_loss: 0.2463\n",
            "Epoch 556/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0227 - val_loss: 0.2113\n",
            "Epoch 557/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0270 - val_loss: 0.2573\n",
            "Epoch 558/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0265 - val_loss: 0.2290\n",
            "Epoch 559/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.2375\n",
            "Epoch 560/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0252 - val_loss: 0.2017\n",
            "Epoch 561/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0299 - val_loss: 0.2448\n",
            "Epoch 562/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0180 - val_loss: 0.2248\n",
            "Epoch 563/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0192 - val_loss: 0.2176\n",
            "Epoch 564/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0214 - val_loss: 0.2932\n",
            "Epoch 565/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.2001\n",
            "Epoch 566/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0218 - val_loss: 0.2654\n",
            "Epoch 567/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0214 - val_loss: 0.1951\n",
            "Epoch 568/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0203 - val_loss: 0.2599\n",
            "Epoch 569/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0268 - val_loss: 0.2167\n",
            "Epoch 570/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0219 - val_loss: 0.2178\n",
            "Epoch 571/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0299 - val_loss: 0.2219\n",
            "Epoch 572/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0151 - val_loss: 0.2159\n",
            "Epoch 573/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0167 - val_loss: 0.2491\n",
            "Epoch 574/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0220 - val_loss: 0.2294\n",
            "Epoch 575/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0185 - val_loss: 0.2364\n",
            "Epoch 576/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0174 - val_loss: 0.2583\n",
            "Epoch 577/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0277 - val_loss: 0.2352\n",
            "Epoch 578/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0219 - val_loss: 0.2369\n",
            "Epoch 579/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0225 - val_loss: 0.2209\n",
            "Epoch 580/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0215 - val_loss: 0.2545\n",
            "Epoch 581/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0305 - val_loss: 0.1995\n",
            "Epoch 582/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0299 - val_loss: 0.3063\n",
            "Epoch 583/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0179 - val_loss: 0.2280\n",
            "Epoch 584/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0214 - val_loss: 0.2597\n",
            "Epoch 585/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0219 - val_loss: 0.1943\n",
            "Epoch 586/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0212 - val_loss: 0.2608\n",
            "Epoch 587/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0157 - val_loss: 0.2298\n",
            "Epoch 588/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0222 - val_loss: 0.2093\n",
            "Epoch 589/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0227 - val_loss: 0.2552\n",
            "Epoch 590/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0263 - val_loss: 0.2431\n",
            "Epoch 591/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0195 - val_loss: 0.2436\n",
            "Epoch 592/600\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.0160 - val_loss: 0.2082\n",
            "Epoch 593/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0245 - val_loss: 0.2540\n",
            "Epoch 594/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0232 - val_loss: 0.2460\n",
            "Epoch 595/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0167 - val_loss: 0.2017\n",
            "Epoch 596/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0176 - val_loss: 0.2448\n",
            "Epoch 597/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0180 - val_loss: 0.2302\n",
            "Epoch 598/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0202 - val_loss: 0.2214\n",
            "Epoch 599/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0283 - val_loss: 0.2359\n",
            "Epoch 600/600\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.0215 - val_loss: 0.2596\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efeb6ea1160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "B75iHgAtWY6G",
        "outputId": "a8cb27b3-c9de-4020-8d2c-bb0b6734f51b"
      },
      "source": [
        "model_loss= pd.DataFrame(model.history.history)\n",
        "model_loss.plot()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7efeaf622fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5qcB0kmWjsT",
        "outputId": "1fd616a0-4be2-4b6e-d3f9-3a6840962306"
      },
      "source": [
        "predicitions=model.predict_classes(X_test) #tries to predict test\n",
        "from sklearn.metrics import classification_report\n",
        "print (classification_report(y_test,predicitions)) #compare real predicition with ai"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.98      0.92        55\n",
            "           1       0.99      0.91      0.95        88\n",
            "\n",
            "    accuracy                           0.94       143\n",
            "   macro avg       0.93      0.95      0.93       143\n",
            "weighted avg       0.94      0.94      0.94       143\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}